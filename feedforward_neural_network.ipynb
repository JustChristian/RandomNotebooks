{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A few helper functions representing a common activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(s):\n",
    "    return s * (1.0 - s)\n",
    "\n",
    "def squared_error(output, target):\n",
    "    return (output - target) ** 2\n",
    "\n",
    "def squared_error_derivative(output, target):\n",
    "    return 2 * (output - target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neuron to be instaniated in NeuralNetwork\n",
    "class Neuron(object):\n",
    "    def __init__(self, input_count):\n",
    "        self.bias = random.random()\n",
    "        self.weights = [random.random() for i in range(input_count)]\n",
    "        self.input_count = input_count\n",
    "    \n",
    "    #Takes the weighted sum of inputs then applies a activation function to normalize the sum\n",
    "    def getOutput(self, inputs):\n",
    "        #TODO: assert(len(inputs) == self.input_count)\n",
    "        weighted_input_sum = self.bias + sum([(self.weights[i] * inputs[i]) for i in range(len(self.weights))])\n",
    "        self.output = sigmoid(weighted_input_sum)\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layer_size_ls):\n",
    "        #Create layers with inputs the size of the previous layers output count\n",
    "        self.layers = []\n",
    "        for i in range(1, len(layer_size_ls)):\n",
    "            layer_size = layer_size_ls[i]\n",
    "            prev_layer_size = layer_size_ls[i-1]\n",
    "            \n",
    "            #Each layer is represented by a list of neurons of a size with inputs large enough for the previous layer\n",
    "            self.layers.append([Neuron(prev_layer_size) for i in range(layer_size)]) \n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        layer_output = inputs\n",
    "        for layer in self.layers:\n",
    "            layer_output = [neuron.getOutput(layer_output) for neuron in layer]\n",
    "        return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackpropagationNNTrainer(object):\n",
    "    def __init__(self, learning_rate=0.2, activation_func=sigmoid, activation_func_derivative=sigmoid_derivative, error_func=squared_error, error_func_derivative=squared_error_derivative):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation_func = activation_func\n",
    "        self.activation_func_derivative = activation_func_derivative\n",
    "        self.error_func = error_func\n",
    "        self.error_func_derivative = error_func_derivative\n",
    "    \n",
    "    #Feed forward training algorithm\n",
    "    def back_propagate(self, network, target):\n",
    "        #1. Compute errors and deltas for the output layer\n",
    "        output_layer = network.layers[-1]\n",
    "        for i in range(len(output_layer)):\n",
    "            neuron = output_layer[i]\n",
    "            neuron.error = self.error_func_derivative(neuron.output, target[i])\n",
    "            neuron.delta = neuron.error * self.activation_func_derivative(neuron.output)\n",
    "\n",
    "        #2. Apply derivative of sigmoid to next layer from each layer (output layer error in the case of last hidden layer)  ... dot-product of weights of each neuron by delta of outputs???\n",
    "        for i in reversed(range(len(network.layers) - 1)):\n",
    "            layer = network.layers[i]\n",
    "            next_layer = network.layers[i+1]\n",
    "\n",
    "            for neuron_i in range(len(layer)):\n",
    "                neuron = layer[neuron_i]\n",
    "                neuron.error = 0.0\n",
    "                for next_neuron in next_layer:\n",
    "                    neuron.error += next_neuron.delta * next_neuron.weights[neuron_i]\n",
    "                neuron.delta = neuron.error * self.activation_func_derivative(neuron.output)\n",
    "        #Revered layers, output -> input\n",
    "            #For each neuron on each layer, go to the next layer get weight from each neuron on that layer and the delta associated\n",
    "\n",
    "    def update_weights(self, network, inputs, learning_rate):\n",
    "        for i in range(len(network.layers)):\n",
    "            curr_layer = network.layers[i]\n",
    "\n",
    "            #outputs of previous layer are the input of this layer\n",
    "            if i != 0:\n",
    "                inputs = [neuron.output for neuron in network.layers[i-1]]\n",
    "\n",
    "            for neuron in curr_layer:\n",
    "                scaled_delta = learning_rate * neuron.delta\n",
    "                for j in range(len(inputs)):\n",
    "                    neuron.weights[j] -= scaled_delta * inputs[j]\n",
    "                neuron.bias -= scaled_delta\n",
    "                \n",
    "    def train(self, network, rows, learning_rate=0.2, n_epoch=1):\n",
    "        #start_time = time.time()\n",
    "        for epoch in tqdm_notebook(range(n_epoch)):\n",
    "            #sum_error = 0.0\n",
    "            for row in rows:\n",
    "                X = row[:-1]\n",
    "                y = row[-1]\n",
    "\n",
    "                output = network.predict(X)\n",
    "                #sum_error += abs(sum(self.computeError(target)))\n",
    "                self.back_propagate(network, y)\n",
    "                self.update_weights(network, X, learning_rate) #Check that this should be inputs passed in\n",
    "            #print(\">epoch %d: %.3f\" % (epoch, sum_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6088b804d5144cd1b1999b2b836b640c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Build an AND dataset\n",
    "dataset = []\n",
    "for i in range(1000):\n",
    "    vals = [random.randint(0,1), random.randint(0,1)]\n",
    "    dataset.append([vals[0], vals[1], [vals[0] and vals[1]]])\n",
    "\n",
    "n = NeuralNetwork([2, 5, 1])\n",
    "trainer = BackpropagationNNTrainer()\n",
    "trainer.train(n, dataset, n_epoch=100)\n",
    "\n",
    "#Test that the network represents an AND gate\n",
    "assert(round(n.predict([0,0])[0]) == 0)\n",
    "assert(round(n.predict([1,0])[0]) == 0)\n",
    "assert(round(n.predict([0,1])[0]) == 0)\n",
    "assert(round(n.predict([1,1])[0]) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503dfef2cfba4e0191ab2e3647b551b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Build an XOR gate dataset, with 4 hidden layer!!!\n",
    "dataset = []\n",
    "for i in range(1000):\n",
    "    vals = [random.randint(0,1), random.randint(0,1)]\n",
    "    dataset.append([vals[0], vals[1], [(not (vals[0] and vals[1])) and (vals[0] or vals[1])]])\n",
    "\n",
    "n = NeuralNetwork([2, 5, 5, 1])\n",
    "trainer = BackpropagationNNTrainer(learning_rate=0.1)\n",
    "trainer.train(n, dataset, n_epoch=200)\n",
    "\n",
    "#Test that the network represents an XOR gate\n",
    "assert(round(n.predict([0,0])[0]) == 0)\n",
    "assert(round(n.predict([1,0])[0]) == 1)\n",
    "assert(round(n.predict([0,1])[0]) == 1)\n",
    "assert(round(n.predict([1,1])[0]) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eefde0ef3a59446a8ba0a3f223a9a57c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Build an AND dataset\n",
    "dataset = []\n",
    "for i in range(1000):\n",
    "    vals = [random.randint(0,1), random.randint(0,1), random.randint(0,1), random.randint(0,1)]\n",
    "    dataset.append([vals[0], vals[1], vals[2], vals[3], [vals[0] and vals[1] and vals[2] and vals[3]]])\n",
    "\n",
    "n = NeuralNetwork([4, 5, 1])\n",
    "trainer = BackpropagationNNTrainer(learning_rate=0.1)\n",
    "trainer.train(n, dataset, n_epoch=200)\n",
    "\n",
    "#Test that the network represents an AND gate\n",
    "assert(round(n.predict([0,0,0,0])[0]) == 0)\n",
    "assert(round(n.predict([0,0,0,1])[0]) == 0)\n",
    "assert(round(n.predict([0,0,1,0])[0]) == 0)\n",
    "assert(round(n.predict([0,0,1,1])[0]) == 0)\n",
    "assert(round(n.predict([0,1,0,0])[0]) == 0)\n",
    "assert(round(n.predict([0,1,0,1])[0]) == 0)\n",
    "assert(round(n.predict([0,1,1,0])[0]) == 0)\n",
    "assert(round(n.predict([0,1,1,1])[0]) == 0)\n",
    "assert(round(n.predict([1,0,0,0])[0]) == 0)\n",
    "assert(round(n.predict([1,0,0,1])[0]) == 0)\n",
    "assert(round(n.predict([1,0,1,0])[0]) == 0)\n",
    "assert(round(n.predict([1,0,1,1])[0]) == 0)\n",
    "assert(round(n.predict([1,1,0,0])[0]) == 0)\n",
    "assert(round(n.predict([1,1,0,1])[0]) == 0)\n",
    "assert(round(n.predict([1,1,1,0])[0]) == 0)\n",
    "assert(round(n.predict([1,1,1,1])[0]) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74e515622214b42bd15b5f698775ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Should be lower than 0.5\n",
      "[9.140601020797631e-09]\n",
      "[3.011579357456661e-11]\n",
      "[0.00010415661262241913]\n",
      "[0.04348022886371581]\n",
      "\n",
      "Should be higher than 0.5\n",
      "[0.9999999593997994]\n",
      "[0.9986217579183516]\n",
      "[0.19174162154598118]\n"
     ]
    }
   ],
   "source": [
    "#Build an SumToOne dataset\n",
    "dataset = []\n",
    "for i in range(1000):\n",
    "    vals = [random.random(), random.random(), ]\n",
    "    dataset.append([vals[0], vals[1], [(vals[0]+vals[1]) >= 1.0]])\n",
    "\n",
    "n = NeuralNetwork([2, 10, 1])\n",
    "trainer = BackpropagationNNTrainer(learning_rate=0.1)\n",
    "trainer.train(n, dataset, n_epoch=200)\n",
    "\n",
    "#Test that the network represents a SumToOne gate\n",
    "print(\"Should be lower than 0.5\")\n",
    "print(n.predict([0.3, 0.4]))\n",
    "print(n.predict([0.1, 0.2]))\n",
    "print(n.predict([0.45, 0.45]))\n",
    "print(n.predict([0.49, 0.49]))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Should be higher than 0.5\")\n",
    "print(n.predict([0.3, 1.0]))\n",
    "print(n.predict([0.6, 0.5]))\n",
    "print(n.predict([0.50, 0.50]))\n",
    "#it's so close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if it's classification maybe I should just round things to bind them into groups???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1d420cbd144b9f9fb33df37549165c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Build an AND dataset\n",
    "dataset = []\n",
    "for i in range(1000):\n",
    "    vals = [random.randint(0,1), random.randint(0,1), random.randint(0,1), random.randint(0,1)]\n",
    "    dataset.append([vals[0], vals[1], vals[2], vals[3], [vals[0] and vals[1] and vals[2] and vals[3]]])\n",
    "\n",
    "n = NeuralNetwork([4, 5, 5, 1])\n",
    "trainer = BackpropagationNNTrainer(learning_rate=0.1)\n",
    "trainer.train(n, dataset, n_epoch=500)\n",
    "\n",
    "#Test that the network represents an AND gate\n",
    "assert(round(n.predict([0,0,0,0])[0]) == 0)\n",
    "assert(round(n.predict([0,0,0,1])[0]) == 0)\n",
    "assert(round(n.predict([0,0,1,0])[0]) == 0)\n",
    "assert(round(n.predict([0,0,1,1])[0]) == 0)\n",
    "assert(round(n.predict([0,1,0,0])[0]) == 0)\n",
    "assert(round(n.predict([0,1,0,1])[0]) == 0)\n",
    "assert(round(n.predict([0,1,1,0])[0]) == 0)\n",
    "assert(round(n.predict([0,1,1,1])[0]) == 0)\n",
    "assert(round(n.predict([1,0,0,0])[0]) == 0)\n",
    "assert(round(n.predict([1,0,0,1])[0]) == 0)\n",
    "assert(round(n.predict([1,0,1,0])[0]) == 0)\n",
    "assert(round(n.predict([1,0,1,1])[0]) == 0)\n",
    "assert(round(n.predict([1,1,0,0])[0]) == 0)\n",
    "assert(round(n.predict([1,1,0,1])[0]) == 0)\n",
    "assert(round(n.predict([1,1,1,0])[0]) == 0)\n",
    "assert(round(n.predict([1,1,1,1])[0]) == 1)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        for k in range(2):\n",
    "            for l in range(2):\n",
    "                print(\"[\", i, j, k, l, \"]\", round(n.predict([i, j, k, l])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maybe the deeper the number of layers the more training that is needed\n",
    "#Weights are not constrained to be positive or even -1.0 to 1.0 they can be any number"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
