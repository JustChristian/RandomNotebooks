{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning Neural Networks\n",
    "import random\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A few helper functions representing a common activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(s):\n",
    "    return s * (1.0 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(object):\n",
    "    def __init__(self, input_count, activation_func=sigmoid):\n",
    "        self.bias = random.random()\n",
    "        self.weights = [random.random() for i in range(input_count)]\n",
    "        self.input_count = input_count\n",
    "        self.activation_func = activation_func\n",
    "    \n",
    "    #Takes the weighted sum of inputs then applies a activation function \n",
    "    #to narmalize the sum\n",
    "    def dot_product(self, inputs):\n",
    "        \"\"\"if len(inputs) != self.input_count:\n",
    "            pass #raise error\"\"\"\n",
    "        weighted_input_sum = self.bias + sum([(self.weights[i] * inputs[i]) for i in range(len(inputs))])\n",
    "        self.output = self.activation_func(weighted_input_sum)\n",
    "        return self.output\n",
    "            \n",
    "def makeNeuronLayer(input_count, neuron_count):\n",
    "    return [Neuron(input_count) for i in range(neuron_count)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(val, max_val):\n",
    "    encoded_val = [0 for i in range(max_val)]\n",
    "    encoded_val[val-1] = 1\n",
    "    return encoded_val\n",
    "\n",
    "#TODO: Add support for regression later\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self, n_inputs, n_hidden_neuron_count, n_outputs, n_hidden_layers, learning_rate=0.2, debug=False):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.debug = debug\n",
    "        \n",
    "        self.layers = []\n",
    "        #input -> hidden\n",
    "        self.layers.append(makeNeuronLayer(n_inputs, n_hidden_neuron_count))\n",
    "        #hidden -> hidden\n",
    "        for i in range(n_hidden_layers):\n",
    "            self.layers.append(makeNeuronLayer(n_hidden_neuron_count, n_hidden_neuron_count))\n",
    "        #hidden -> output\n",
    "        self.layers.append(makeNeuronLayer(n_hidden_neuron_count, n_outputs))\n",
    "        \n",
    "    def backprob(self, target):\n",
    "        \"\"\"target = one_hot_encode(target, self.n_outputs)\n",
    "        \n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            layer = self.layers[i]\n",
    "            errors = []\n",
    "            if i != len(self.layers) - 1:\n",
    "                for j in range(len(layer)):\n",
    "                    neuron_error = 0.0\n",
    "                    for neuron in self.layers[i + 1]:\n",
    "                        neuron_error += (neuron.weights[j] * neuron.delta)\n",
    "                    errors.append(neuron_error)\n",
    "            else:\n",
    "                for j in range(len(layer)):\n",
    "                    neuron = layer[j]\n",
    "                    errors.append(target[j] - neuron.output)\n",
    "            \n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                neuron.delta = errors[j] * sigmoid_derivative(neuron.output)\"\"\"\n",
    "        target = one_hot_encode(target, self.n_outputs)\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            layer = self.layers[i]\n",
    "            errors = []\n",
    "            if i != (len(self.layers) - 1):\n",
    "                for j in range(len(layer)):\n",
    "                    error = 0.0\n",
    "                    for neuron in self.layers[i + 1]:\n",
    "                        error += (neuron.weights[j] * neuron.delta)\n",
    "                    errors.append(error)\n",
    "            else:\n",
    "                for j in range(len(layer)):\n",
    "                    neuron = layer[j]\n",
    "                    #error = 0.5 * (target[j] - neuron.output)**2\n",
    "                    error = neuron.output - target[j]\n",
    "                    \"\"\"print(\"STEP\")\n",
    "                    print(\"\\ttarget:\", target[j])\n",
    "                    print(\"\\toutput:\", neuron.output)\n",
    "                    print(\"\\terror:\", error)\"\"\"\n",
    "                    errors.append(error)\n",
    "                    \n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                neuron.delta = errors[j] * sigmoid_derivative(neuron.output)\n",
    "\n",
    "    def update_weights(self, inputs):\n",
    "        for i in range(len(self.layers)):\n",
    "            if i != 0:\n",
    "                inputs = [neuron.output for neuron in self.layers[i - 1]]\n",
    "                \n",
    "            for neuron in self.layers[i]:\n",
    "                for j in range(len(inputs)):\n",
    "                    neuron.weights[j] += self.learning_rate * neuron.delta * inputs[j]\n",
    "                neuron.bias += self.learning_rate * neuron.delta\n",
    "\n",
    "    def train(self, rows, n_epoch=1):\n",
    "        \"\"\"for epoch in range(n_epoch):\n",
    "            sum_error = 0.0\n",
    "            for row in rows:\n",
    "                inputs = row[0:-1]\n",
    "                target = row[-1]\n",
    "\n",
    "                output = self.predict(inputs)\n",
    "                sum_error += (target - output)**2\n",
    "                \n",
    "                self.backprob(output)\n",
    "                self.update_weights(inputs)\n",
    "            print(\">Epoch: %d %.3f\" % (epoch, sum_error))\n",
    "            \n",
    "        #Accuracy test, after training\n",
    "        corr_count = 0\n",
    "        for row in rows:\n",
    "            inputs = row[0:-1]\n",
    "            target = row[-1]\n",
    "            output = self.predict(inputs)\n",
    "            if output is target:\n",
    "                corr_count += 1\n",
    "            print(output, \"--\", target)\n",
    "        print(\"Accuracy:\", corr_count/float(len(rows)))\"\"\"\n",
    "        for epoch in range(n_epoch):\n",
    "            sum_error = 0\n",
    "            for row in rows:\n",
    "                inputs = row[0:-1]\n",
    "                target = row[-1]\n",
    "                output = self.predict(inputs)\n",
    "                \n",
    "                sum_error += 0.5 * (output - target)**2\n",
    "                #sum_error = (target-output)\n",
    "                \n",
    "                self.backprob(target)\n",
    "                self.update_weights(inputs)\n",
    "            print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, self.learning_rate, sum_error))\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = [neuron.dot_product(inputs) for neuron in layer]\n",
    "        return inputs.index(max(inputs)) #This forces classification, works great with one-hot encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "#Helper method for getting all the rows of a .csv\n",
    "def read_csv_data(filename, skip_first=False):\n",
    "    lines = []\n",
    "    with open(filename, newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        if skip_first: #Skips the first row, possibly due to header\n",
    "            next(reader, None)\n",
    "        lines = [row for row in reader]\n",
    "    return lines\n",
    "\n",
    "#Attempt to converts the given columns of a list to floats\n",
    "def string_columns_to_floats(rows, column_ls):\n",
    "    for ls in rows:\n",
    "        for i in column_ls:\n",
    "            try:\n",
    "                ls[i] = float(ls[i])\n",
    "            except ValueError:\n",
    "                pass\n",
    "            \n",
    "#takes columns that are classification represented as string and converts them to a number classifications\n",
    "def number_encode_classification(rows, column_ls):\n",
    "    for column in column_ls:\n",
    "        assoc_values = {}\n",
    "        value_count = 0\n",
    "\n",
    "        for row in rows:\n",
    "            if row[column] not in assoc_values:\n",
    "                assoc_values[row[column]] = value_count\n",
    "                value_count += 1\n",
    "            row[column] = assoc_values[row[column]]\n",
    "            \n",
    "#Moves all list items into random positions\n",
    "def shuffle_list_items(rows):\n",
    "    return random.sample(rows, len(rows))\n",
    "\n",
    "data = read_csv_data(\"datasets/iris.csv\", skip_first=True)\n",
    "\n",
    "string_columns_to_floats(data, [0,1,2,3])\n",
    "number_encode_classification(data, [4])\n",
    "data = shuffle_list_items(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5620888731389184\n",
      "0.3551047099196044\n",
      "0.058944465291167925\n",
      ">epoch=0, lrate=0.200, error=56.000\n",
      ">epoch=1, lrate=0.200, error=53.000\n",
      ">epoch=2, lrate=0.200, error=53.500\n",
      ">epoch=3, lrate=0.200, error=52.000\n",
      ">epoch=4, lrate=0.200, error=52.000\n",
      ">epoch=5, lrate=0.200, error=50.000\n",
      ">epoch=6, lrate=0.200, error=50.000\n",
      ">epoch=7, lrate=0.200, error=50.000\n",
      ">epoch=8, lrate=0.200, error=50.000\n",
      ">epoch=9, lrate=0.200, error=50.000\n",
      ">epoch=10, lrate=0.200, error=50.000\n",
      ">epoch=11, lrate=0.200, error=50.000\n",
      ">epoch=12, lrate=0.200, error=50.000\n",
      ">epoch=13, lrate=0.200, error=50.000\n",
      ">epoch=14, lrate=0.200, error=50.000\n",
      ">epoch=15, lrate=0.200, error=50.000\n",
      ">epoch=16, lrate=0.200, error=50.000\n",
      ">epoch=17, lrate=0.200, error=50.000\n",
      ">epoch=18, lrate=0.200, error=50.000\n",
      ">epoch=19, lrate=0.200, error=50.000\n",
      ">epoch=20, lrate=0.200, error=50.000\n",
      ">epoch=21, lrate=0.200, error=50.000\n",
      ">epoch=22, lrate=0.200, error=50.000\n",
      ">epoch=23, lrate=0.200, error=50.000\n",
      ">epoch=24, lrate=0.200, error=50.000\n",
      ">epoch=25, lrate=0.200, error=50.000\n",
      ">epoch=26, lrate=0.200, error=50.000\n",
      ">epoch=27, lrate=0.200, error=50.000\n",
      ">epoch=28, lrate=0.200, error=50.000\n",
      ">epoch=29, lrate=0.200, error=50.000\n",
      ">epoch=30, lrate=0.200, error=50.000\n",
      ">epoch=31, lrate=0.200, error=50.000\n",
      ">epoch=32, lrate=0.200, error=50.000\n",
      ">epoch=33, lrate=0.200, error=50.000\n",
      ">epoch=34, lrate=0.200, error=50.000\n",
      ">epoch=35, lrate=0.200, error=50.000\n",
      ">epoch=36, lrate=0.200, error=50.000\n",
      ">epoch=37, lrate=0.200, error=50.000\n",
      ">epoch=38, lrate=0.200, error=50.000\n",
      ">epoch=39, lrate=0.200, error=50.000\n",
      ">epoch=40, lrate=0.200, error=50.000\n",
      ">epoch=41, lrate=0.200, error=50.000\n",
      ">epoch=42, lrate=0.200, error=50.000\n",
      ">epoch=43, lrate=0.200, error=50.000\n",
      ">epoch=44, lrate=0.200, error=50.000\n",
      ">epoch=45, lrate=0.200, error=50.000\n",
      ">epoch=46, lrate=0.200, error=50.000\n",
      ">epoch=47, lrate=0.200, error=50.000\n",
      ">epoch=48, lrate=0.200, error=50.000\n",
      ">epoch=49, lrate=0.200, error=50.000\n",
      "0.578684118940349\n",
      "0.3707374123887064\n",
      "0.0755770304929149\n"
     ]
    }
   ],
   "source": [
    "#Some tests that fill the neural network with known good data to ensure that proper outputs occur\n",
    "n1 = NeuralNetwork(4, 5, 3, 2)\n",
    "print(n1.layers[1][0].weights[0])\n",
    "print(n1.layers[1][0].weights[1])\n",
    "print(n1.layers[1][0].weights[2])\n",
    "n1.train(data, n_epoch=50)\n",
    "print(n1.layers[1][0].weights[0])\n",
    "print(n1.layers[1][0].weights[1])\n",
    "print(n1.layers[1][0].weights[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7513650695523157 == 0.751\n",
      "0.7729284653214625 == 0.772\n",
      "0.18681560180895948 == 0.1868\n",
      "[0.4, 0.45]\n",
      "[0.5, 0.55]\n"
     ]
    }
   ],
   "source": [
    "#Example link - https://matthewmazur.files.wordpress.com/2018/03/neural_network-9.png\n",
    "#https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "n2 = NeuralNetwork(2, 2, 2, 1, debug=True)\n",
    "n2.layers[0][0].weights = [.15, .20]\n",
    "n2.layers[0][1].weights = [.25, .30]\n",
    "n2.layers[1][0].weights = [.40, .45]\n",
    "n2.layers[1][1].weights = [.50, .55]\n",
    "n2.layers[0][0].bias = 0.35\n",
    "n2.layers[0][1].bias = 0.35\n",
    "n2.layers[1][0].bias = 0.60\n",
    "n2.layers[1][1].bias = 0.60\n",
    "\n",
    "n2.predict([0.05, 0.10])\n",
    "print(n2.layers[1][0].output, \"==\", 0.751)\n",
    "print(n2.layers[1][1].output, \"==\", 0.772)\n",
    "\n",
    "print(sigmoid_derivative(n2.layers[1][0].output), \"==\", 0.1868)\n",
    "\n",
    "n2.backprob(1)\n",
    "output_layer = n2.layers[1]\n",
    "print(output_layer[0].weights)\n",
    "print(output_layer[1].weights)\n",
    "#Checks out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
