{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1.0 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(object):\n",
    "    def __init__(self, prev_layer_size, activation_func):\n",
    "        #Make random weights and bias for neuron\n",
    "        self.weights = [random.random() for i in range(prev_layer_size)]\n",
    "        self.bias = random.random()\n",
    "        self.activation_func = activation_func\n",
    "        \n",
    "    def forward(self, X):\n",
    "        activation_sum = self.bias\n",
    "        for i in range(len(X)):\n",
    "            activation_sum += self.weights[i] * X[i]\n",
    "        self.output = self.activation_func(activation_sum)\n",
    "        return self.output\n",
    "\n",
    "class NeuronLayer(object):\n",
    "    def __init__(self, layer_size, prev_layer_size, activation_func=sigmoid):\n",
    "        self.neurons = [Neuron(prev_layer_size, activation_func) for i in range(layer_size)]\n",
    "        \n",
    "    #Returns the output of each neuron for the given outputs of the previous layer\n",
    "    def forward(self, X):\n",
    "        return [neuron.forward(X) for neuron in self.neurons]\n",
    "    \n",
    "    \n",
    "class FeedForwardNeuralNetwork(object):\n",
    "    def __init__(self, input_size, hidden_size, output_size, hidden_layer_count=1, learning_rate=0.1, activation_func=sigmoid, activation_func_derivative=sigmoid_derivative):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layer_count = hidden_layer_count\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.activation_func = activation_func\n",
    "        self.activation_func_derivative = activation_func_derivative\n",
    "        \n",
    "        #Make neural network\n",
    "        self.network = []\n",
    "        #input -> hidden layer\n",
    "        self.network.append(NeuronLayer(self.hidden_size, self.input_size))\n",
    "        #hidden -> hidden layers\n",
    "        self.network.extend([NeuronLayer(self.hidden_size, self.hidden_size) for i in range(self.hidden_layer_count)])\n",
    "        #hidden -> output layer\n",
    "        self.network.append(NeuronLayer(self.output_size, self.hidden_size))\n",
    "        \n",
    "    def output_to_final(self, neuron_vals):\n",
    "        \n",
    "        \n",
    "    ###CHECKED\n",
    "    #Forward propagation through the network\n",
    "    def forward(self, X):\n",
    "        #Forward from each layer to the next layer\n",
    "        #use the output of each layer as the input to the next layer\n",
    "        \"\"\"for layer in self.network:\n",
    "            X = layer.forward(X)\n",
    "        return X\"\"\"\n",
    "        inputs = X\n",
    "        for layer in self.network:\n",
    "            new_inputs = []\n",
    "            for neuron in layer.neurons:\n",
    "                input_sum = neuron.bias\n",
    "                input_sum += sum([inputs[i] * neuron.weights[i] for i in range(len(neuron.weights))])\n",
    "                \n",
    "                neuron.output = self.activation_func(input_sum)\n",
    "                new_inputs.append(neuron.output)\n",
    "            inputs = new_inputs\n",
    "        return output_to_final(inputs)\n",
    "        \n",
    "    ###CHECKED\n",
    "    def back_propagate_error(self, expected): #target):\n",
    "        \"\"\"prev_layer = None\n",
    "        \n",
    "        #Back propagate error through all hidden\n",
    "        for layer in reversed(self.network):\n",
    "            errors = []\n",
    "            \n",
    "            if layer is self.network[-1]: #Compute error of output layer\n",
    "                errors = [(target[i] - layer.neurons[i].output) for i in range(len(layer.neurons))]\n",
    "            else: #Computer error of every other layer\n",
    "                for i in range(len(layer.neurons)): # For neuron\n",
    "                    error_total = 0.0\n",
    "                    for j in range(len(prev_layer.neurons)): #Take the error of each neuron in the next layer\n",
    "                        neuron = prev_layer.neurons[j]\n",
    "                        #... and back propagate a portion of the error in accordance with the weight of the neuron attachment\n",
    "                        error_total += neuron.weights[i] * neuron.delta\n",
    "                    #And give error to the neuron it belongs to\n",
    "                    errors.append(error_total)\n",
    "            \n",
    "            for i in range(len(layer.neurons)):\n",
    "                neuron = layer.neurons[i]\n",
    "                neuron.delta = errors[i] * self.activation_func_derivative(neuron.output)\n",
    "                    \n",
    "            prev_layer = layer\"\"\"\n",
    "        for i in reversed(range(len(self.network))):\n",
    "            layer = self.network[i]\n",
    "            errors = []\n",
    "            if i != len(self.network) - 1:\n",
    "                for j in range(len(layer.neurons)):\n",
    "                    error = 0.0\n",
    "                    for neuron in self.network[i + 1].neurons:\n",
    "                        error += (neuron.weights[j] * neuron.delta)\n",
    "                    errors.append(error)\n",
    "            else:\n",
    "                for j in range(len(layer.neurons)):\n",
    "                    neuron = layer.neurons[j]\n",
    "                    errors.append(expected - output_to_final(neuron.output))\n",
    "            \n",
    "            for j in range(len(layer.neurons)):\n",
    "                neuron = layer.neurons[j]\n",
    "                neuron.delta = errors[j] * self.activation_func_derivative(neuron.output)\n",
    "          \n",
    "    ###CHECKED\n",
    "    def update_weights(self, inputs):\n",
    "        '''inputs = input_vals\n",
    "        \n",
    "        for layer in self.network:\n",
    "            for neuron in layer.neurons:\n",
    "                for j in range(len(neuron.weights)):\n",
    "                    neuron.weights[j] += self.learning_rate * neuron.delta * inputs[j]\n",
    "                neuron.bias += self.learning_rate * neuron.delta\n",
    "                    \n",
    "            inputs = layer.forward(inputs)'''\n",
    "        for i in range(len(self.network)):\n",
    "            if i != 0:\n",
    "                inputs = [neuron.output for neuron in self.network[i-1].neurons]\n",
    "            \n",
    "            for neuron in self.network[i].neurons:\n",
    "                for j in range(len(inputs)):\n",
    "                    neuron.weights[j] += self.learning_rate * neuron.delta * inputs[j]\n",
    "                neuron.bias += self.learning_rate * neuron.delta\n",
    "            \n",
    "        \n",
    "                \n",
    "            \n",
    "    \"\"\"def train(self, X, targets, n_epoch=1):\n",
    "        for i in range(n_epoch):\n",
    "            sum_error = 0.0\n",
    "            for j in range(len(X)):\n",
    "                output = self.forward(X[j])\n",
    "                expected = targets[j]\n",
    "                \n",
    "                single_error = sum([(expected[t] - output[t])**2 for t in range(len(expected))])\n",
    "                #print(X[j], \",\", expected, \",\", output, \"==\", single_error)\n",
    "                sum_error += single_error\n",
    "\n",
    "                self.back_propagate_error(expected)\n",
    "                self.update_weights(X[j])\n",
    "            print(\"Epoch: %d, error=%.3f\" % (i, sum_error))\"\"\"\n",
    "    #CHECKED\n",
    "    def train(self, X, targets, n_epoch=1):\n",
    "        for epoch in range(n_epoch):\n",
    "            sum_error = 0\n",
    "            for i in range(len(X)):\n",
    "                row = X[i]\n",
    "                \n",
    "                outputs = self.forward(row)\n",
    "                expected = targets[i]\n",
    "\n",
    "                #sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "                sum_error += (expected - outputs)**2\n",
    "                self.back_propagate_error(expected)\n",
    "                self.update_weights(row)\n",
    "            print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, self.learning_rate, sum_error))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification problems, best results are achieved when the network has one \n",
    "neuron in the output layer for each class value. For example, a 2-class or binary \n",
    "classification problem with the class values of A and B. These expected outputs \n",
    "would have to be transformed into binary vectors with one column for each class \n",
    "value. Such as [1, 0] and [0, 1] for A and B respectively. This is called a one \n",
    "hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts from string to integers and makes all integers that are the same equal to the same integer\n",
    "def stringsToNumbers(ls):\n",
    "    val_assocs = {}\n",
    "    vals = 0\n",
    "    \n",
    "    output_ls = []\n",
    "    for row in ls:\n",
    "        if row not in val_assocs:\n",
    "            val_assocs[row] = vals\n",
    "            vals += 1\n",
    "        output_ls.append(val_assocs[row])\n",
    "    \n",
    "    return (output_ls, vals)\n",
    "\n",
    "def one_hot_encode_ls(vals):\n",
    "    vals_possible = len(set(vals))\n",
    "    hot_ls = []\n",
    "    for val in vals:\n",
    "        hot_ls.append([((val == i) if 1 else 0) for i in range(vals_possible)])\n",
    "    return (hot_ls, vals_possible)\n",
    "\n",
    "def shake_up_data(ls):\n",
    "    return random.sample(ls, len(ls))\n",
    "\n",
    "data = []\n",
    "targets = []\n",
    "val_possible = None\n",
    "        \n",
    "with open('datasets/iris.csv', 'r', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader, None)\n",
    "    \n",
    "    rows = [row for row in reader]\n",
    "        \n",
    "\n",
    "rows = shake_up_data(rows)\n",
    "for row in rows:\n",
    "    data.append([float(row[0]), float(row[1]), float(row[2]), float(row[3])])\n",
    "    targets.append(row[4])\n",
    "\n",
    "targets, vals_possible = stringsToNumbers(targets)\n",
    "#targets, vals_possible = one_hot_encode_ls(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... <class 'float'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-424-4d30df8a7114>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvals_possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedForwardNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layer_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.03\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-422-74c6170b6105>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, targets, n_epoch)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;31m#sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0msum_error\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexpected\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_propagate_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>epoch=%d, lrate=%.3f, error=%.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-422-74c6170b6105>\u001b[0m in \u001b[0;36mback_propagate_error\u001b[0;34m(self, expected)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                     \u001b[0mneuron\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mneuron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mneuron\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(\"...\", type(data[0][0]))\n",
    "\n",
    "input_size = len(data[0])\n",
    "output_size = vals_possible\n",
    "n = FeedForwardNeuralNetwork(input_size, 6, output_size, hidden_layer_count=1, learning_rate=0.03)\n",
    "n.train(data, targets, n_epoch=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8552062839734539, 0.9764870360512872, 0.9734452792250252] == [True, False, False]\n",
      "[0.8550814523792525, 0.9764288389542414, 0.9733496375678469] == [False, True, False]\n",
      "[0.8550679919619117, 0.976422039734102, 0.9733391309299845] == [False, True, False]\n",
      "[0.8550701205430431, 0.9764227195177327, 0.9733406521915873] == [False, True, False]\n",
      "[0.8550679919619117, 0.976422039734102, 0.9733391309299845] == [False, True, False]\n",
      "[0.8552126532220969, 0.9764900004994941, 0.9734501543585938] == [False, False, True]\n",
      "[0.8552132352232048, 0.97649028533108, 0.9734506041867991] == [False, False, True]\n",
      "[0.8550829776663206, 0.9764289239728748, 0.9733505641492398] == [False, True, False]\n",
      "[0.8552143839710857, 0.9764908217963585, 0.9734514845052237] == [False, False, True]\n",
      "[0.855212371181863, 0.9764898933652566, 0.9734499469963049] == [False, False, True]\n",
      "[0.8552130649044294, 0.976490217335181, 0.9734504781397075] == [False, False, True]\n",
      "[0.855203041826034, 0.9764854907193651, 0.9734427854850924] == [True, False, False]\n",
      "[0.8552055524637586, 0.9764866971344291, 0.9734447204892284] == [True, False, False]\n",
      "[0.8550775193322889, 0.9764270358906579, 0.9733466077674423] == [False, True, False]\n",
      "[0.8552142840810558, 0.9764907869863907, 0.9734514118108656] == [False, False, True]\n",
      "[0.8551882088779634, 0.9764780916660779, 0.9734312580202861] == [True, False, False]\n",
      "[0.8552126807154828, 0.9764900261698152, 0.973450179956142] == [False, False, True]\n",
      "[0.8551043941212092, 0.9764394220485046, 0.9733671480368] == [False, True, False]\n",
      "[0.8552073379403705, 0.9764875402788341, 0.9734460913985954] == [True, False, False]\n",
      "[0.8552070429923159, 0.9764872968101731, 0.9734458305573941] == [True, False, False]\n",
      "[0.8551229851255607, 0.9764485815044763, 0.9733815857827156] == [False, True, False]\n",
      "[0.8552108496031926, 0.9764891282411238, 0.9734487632041454] == [False, False, True]\n",
      "[0.8552054649699175, 0.9764865812356885, 0.9734446293886433] == [True, False, False]\n",
      "[0.8550833378059921, 0.9764295885898956, 0.9733510194223952] == [False, True, False]\n",
      "[0.8552126476699525, 0.9764900041559077, 0.9734501524755581] == [False, False, True]\n",
      "[0.855208286253812, 0.9764879611384217, 0.9734468102374088] == [True, False, False]\n",
      "[0.855121947491313, 0.9764474675705659, 0.9733805654455252] == [False, True, False]\n",
      "[0.8552130152750758, 0.9764901881791922, 0.973450437900307] == [False, False, True]\n",
      "[0.8550859323224481, 0.9764317596724664, 0.9733533870061213] == [False, True, False]\n",
      "[0.8552056242809939, 0.9764867069274523, 0.9734447656934749] == [True, False, False]\n",
      "[0.855079511557614, 0.97642795281744, 0.9733481440419253] == [False, True, False]\n",
      "[0.8552091064624541, 0.9764883714240271, 0.9734474466177953] == [True, False, False]\n",
      "[0.8550797954903376, 0.976427855442765, 0.9733482647582028] == [False, True, False]\n",
      "[0.8552133441436841, 0.9764903315040561, 0.9734506863817016] == [False, False, True]\n",
      "[0.8550534746692244, 0.9764139302862381, 0.9733274716398326] == [False, True, False]\n",
      "[0.8550892842831826, 0.9764324228865634, 0.9733555977146423] == [False, True, False]\n",
      "[0.855075418357495, 0.9764257238273734, 0.9733448936579931] == [False, True, False]\n",
      "[0.8551207922063946, 0.9764476880777896, 0.9733799697461754] == [False, True, False]\n",
      "[0.8552137388932172, 0.9764905197128174, 0.9734509897163027] == [False, False, True]\n",
      "[0.8550889768356582, 0.9764324807661514, 0.9733554464042674] == [False, True, False]\n",
      "[0.8550858805258382, 0.9764308620417244, 0.9733530078128931] == [False, True, False]\n",
      "[0.855201163896119, 0.9764845158043663, 0.9734413116428738] == [True, False, False]\n",
      "[0.8552088941961323, 0.9764882595477793, 0.9734472810637821] == [True, False, False]\n",
      "[0.8551942428279441, 0.9764812402598256, 0.9734359959546629] == [True, False, False]\n",
      "[0.8552008139513321, 0.9764843463631556, 0.9734410380648636] == [True, False, False]\n",
      "[0.8552027471983241, 0.9764853360322893, 0.9734425521479587] == [True, False, False]\n",
      "[0.8552128408927322, 0.9764900867035835, 0.9734502975964473] == [False, False, True]\n",
      "[0.8552104965737346, 0.9764889958151207, 0.973448503304773] == [False, False, True]\n",
      "[0.855209856454157, 0.9764886896880334, 0.9734480099913141] == [False, False, True]\n",
      "[0.8550974679269653, 0.9764365319718525, 0.9733619633546501] == [False, True, False]\n",
      "[0.8551843573673447, 0.9764764015282611, 0.9734283397668789] == [True, False, False]\n",
      "[0.8551930656013739, 0.9764807484659599, 0.9734351144509421] == [True, False, False]\n",
      "[0.8551985687830814, 0.9764833579833931, 0.9734393408849474] == [True, False, False]\n",
      "[0.8550776791726802, 0.9764260518189043, 0.9733463616870919] == [False, True, False]\n",
      "[0.8552135523769846, 0.9764904350635562, 0.9734508479792613] == [False, False, True]\n",
      "[0.8552140765613048, 0.9764906836410767, 0.973451250861062] == [False, False, True]\n",
      "[0.8552103253601853, 0.976488897299048, 0.9734483661899125] == [False, False, True]\n",
      "[0.8552121601374476, 0.9764897437928066, 0.9734497669177871] == [False, False, True]\n",
      "[0.8552136027703247, 0.9764904565681232, 0.9734508858220016] == [False, False, True]\n",
      "[0.855107581692542, 0.9764407982122166, 0.9733695582961733] == [False, True, False]\n",
      "[0.8551054675860628, 0.9764402526721504, 0.9733681267371256] == [False, True, False]\n",
      "[0.8550700014466246, 0.9764225058128253, 0.973340464452148] == [False, True, False]\n",
      "[0.8552117410367659, 0.976489587105701, 0.9734494598890183] == [False, False, True]\n",
      "[0.8552062384970344, 0.9764869834680282, 0.9734452306780572] == [True, False, False]\n",
      "[0.8552105867881313, 0.976489035165312, 0.9734485707323889] == [False, False, True]\n",
      "[0.8552026484408563, 0.9764853321973568, 0.9734424903658311] == [True, False, False]\n",
      "[0.8552065915141129, 0.9764871572783341, 0.9734455050872949] == [True, False, False]\n",
      "[0.8550517394687088, 0.9764116025363755, 0.973325681594816] == [False, True, False]\n",
      "[0.8552124319408022, 0.9764898966996539, 0.9734499851355429] == [False, False, True]\n",
      "[0.85521405744715, 0.9764906734479141, 0.9734512358480906] == [False, False, True]\n",
      "[0.855082711059648, 0.9764297165294543, 0.9733507235811014] == [False, True, False]\n",
      "[0.8550754531096005, 0.9764253129745898, 0.9733447123478478] == [False, True, False]\n",
      "[0.8552107004708149, 0.9764891028489686, 0.9734486638922504] == [True, False, False]\n",
      "[0.8552140712815429, 0.9764906830520721, 0.9734512472957039] == [False, False, True]\n",
      "[0.8551982910494803, 0.9764831881512872, 0.9734391165755453] == [True, False, False]\n",
      "[0.8552030893357921, 0.9764854798666697, 0.9734428085989787] == [True, False, False]\n",
      "[0.8552133584114462, 0.9764903451370656, 0.9734506999764834] == [False, False, True]\n",
      "[0.8550479987120388, 0.9764117813259316, 0.9733233878057692] == [False, True, False]\n",
      "[0.8552106337107379, 0.9764890067794918, 0.9734485895778306] == [False, False, True]\n",
      "[0.8551859387976781, 0.9764771185075714, 0.9734295449400474] == [True, False, False]\n",
      "[0.8552126993707639, 0.9764900291751828, 0.9734501919204442] == [False, False, True]\n",
      "[0.8551988770854545, 0.9764834479526227, 0.9734395647816649] == [True, False, False]\n",
      "[0.8550930381801841, 0.9764350360172969, 0.9733587941166687] == [False, True, False]\n",
      "[0.8552098059823879, 0.9764886596123655, 0.9734479694822145] == [True, False, False]\n",
      "[0.8550679919619117, 0.976422039734102, 0.9733391309299845] == [False, True, False]\n",
      "[0.8552031721941767, 0.9764855167395736, 0.9734428712705238] == [True, False, False]\n",
      "[0.8551968109416554, 0.9764824459027397, 0.9734379652151898] == [True, False, False]\n",
      "[0.8550851604837645, 0.9764303006431895, 0.9733523491367723] == [False, True, False]\n",
      "[0.8550948014249137, 0.976435396417016, 0.9733599632296834] == [False, True, False]\n",
      "[0.8550629846356813, 0.9764192691258962, 0.9733350894995301] == [False, True, False]\n",
      "[0.8552079402931536, 0.9764878266062814, 0.9734465536336127] == [True, False, False]\n",
      "[0.8552107294502106, 0.9764890770604162, 0.9734486716596362] == [False, False, True]\n",
      "[0.8552052210370202, 0.9764865118120145, 0.9734444542286209] == [True, False, False]\n",
      "[0.8552127756241676, 0.9764900700949425, 0.9734502518576923] == [False, False, True]\n",
      "[0.8550824752299284, 0.9764300144133014, 0.9733506515097131] == [False, True, False]\n",
      "[0.8550589215981556, 0.9764177387527415, 0.9733321524918064] == [False, True, False]\n",
      "[0.8552139507435821, 0.976490621799992, 0.9734511535500224] == [False, False, True]\n",
      "[0.8552118810990131, 0.9764896453376661, 0.9734495648640473] == [False, False, True]\n",
      "[0.8552127525651037, 0.976490056670372, 0.9734502339337425] == [False, False, True]\n",
      "[0.8551055315374314, 0.9764401335998947, 0.973368086433829] == [False, True, False]\n",
      "[0.8552097519987947, 0.9764886515756653, 0.9734479343728869] == [False, False, True]\n",
      "[0.8550948053474595, 0.9764354885092159, 0.9733600224420743] == [False, True, False]\n",
      "[0.8552107294502106, 0.9764890770604162, 0.9734486716596362] == [False, False, True]\n",
      "[0.8552076107077895, 0.9764876519352381, 0.9734462939682008] == [True, False, False]\n",
      "[0.8552116588937133, 0.9764895297349608, 0.9734493908036231] == [False, False, True]\n",
      "[0.8552090307875925, 0.9764883351370535, 0.9734473891644694] == [True, False, False]\n",
      "[0.8552133919523551, 0.9764903595963069, 0.9734507249923696] == [False, False, True]\n",
      "[0.8552099850709451, 0.9764887403080709, 0.973448105940121] == [False, False, True]\n",
      "[0.8550559142297814, 0.9764160196555716, 0.9733297326141376] == [False, True, False]\n",
      "[0.8551101969735809, 0.9764422517634713, 0.9733716712504805] == [False, True, False]\n",
      "[0.8551010810400677, 0.9764383500139696, 0.973364793673258] == [False, True, False]\n",
      "[0.8552133574862666, 0.9764903439312783, 0.9734506988725072] == [False, False, True]\n",
      "[0.8552071247493914, 0.9764874398154004, 0.9734459277035145] == [True, False, False]\n",
      "[0.8551200477114124, 0.976447551023305, 0.9733794603148155] == [False, True, False]\n",
      "[0.8552080253965175, 0.9764878662936832, 0.973446617646004] == [True, False, False]\n",
      "[0.8552010655816527, 0.9764845239981242, 0.9734412562443735] == [True, False, False]\n",
      "[0.8552071935617506, 0.9764874432275064, 0.973445969471371] == [True, False, False]\n",
      "[0.8552035010261749, 0.9764857081424527, 0.9734431361718707] == [True, False, False]\n",
      "[0.8552072312306388, 0.9764874812449411, 0.973446005360314] == [True, False, False]\n",
      "[0.8550924913108249, 0.9764343535435054, 0.9733582227873873] == [False, True, False]\n",
      "[0.8552104135862445, 0.9764889210800822, 0.9734484261725845] == [False, False, True]\n",
      "[0.8552126685378562, 0.9764900095558725, 0.9734501669021894] == [False, False, True]\n",
      "[0.8552119524472968, 0.9764896818667952, 0.9734496204314036] == [False, False, True]\n",
      "[0.8550457890988686, 0.9764103627265034, 0.9733215729877168] == [False, True, False]\n",
      "[0.8552089711766703, 0.9764882586529363, 0.9734473272756637] == [True, False, False]\n",
      "[0.8551997288973219, 0.9764839445458918, 0.9734402472450423] == [True, False, False]\n",
      "[0.8550313293899877, 0.9764046427086083, 0.973310827380539] == [False, True, False]\n",
      "[0.8550873746107381, 0.9764310619540633, 0.9733539881217189] == [False, True, False]\n",
      "[0.855213168548759, 0.9764902497555304, 0.9734505518965633] == [False, False, True]\n",
      "[0.8552051688162514, 0.976486511842304, 0.9734444240452496] == [True, False, False]\n",
      "[0.8552118799575997, 0.9764896558083659, 0.9734495675328426] == [False, False, True]\n",
      "[0.8550084391440278, 0.9763922592952348, 0.9732926676537842] == [False, True, False]\n",
      "[0.8552026733180518, 0.9764853252996358, 0.9734425024918753] == [True, False, False]\n",
      "[0.8552098266471279, 0.9764886865645128, 0.9734479903976679] == [True, False, False]\n",
      "[0.8552003736178453, 0.9764842120773258, 0.9734407296351453] == [True, False, False]\n",
      "[0.855127061486645, 0.9764503171142835, 0.9733846382676602] == [False, True, False]\n",
      "[0.8550578773092283, 0.976416925501128, 0.9733311997811616] == [False, True, False]\n",
      "[0.8552073272761762, 0.9764875440537744, 0.9734460860738603] == [True, False, False]\n",
      "[0.855213970310831, 0.9764906437742109, 0.9734511726692897] == [False, False, True]\n",
      "[0.8550810930324715, 0.9764280566685389, 0.9733491078557475] == [False, True, False]\n",
      "[0.8552102452761704, 0.9764888757068709, 0.973448309521054] == [False, False, True]\n",
      "[0.8552089392655262, 0.9764883067525202, 0.973447324127249] == [True, False, False]\n",
      "[0.8552098866801834, 0.976488737697202, 0.9734480457879982] == [True, False, False]\n",
      "[0.8552062583292785, 0.9764868795815124, 0.9734452047914461] == [False, False, True]\n",
      "[0.8552087729674513, 0.9764881296665996, 0.973447163003322] == [False, False, True]\n",
      "[0.8552133971552194, 0.9764903671776265, 0.9734507308817145] == [False, False, True]\n",
      "[0.8551231327364613, 0.9764480244265858, 0.9733814589154209] == [False, True, False]\n",
      "[0.8552119224733288, 0.9764896728880815, 0.973449599305869] == [False, False, True]\n",
      "[0.855185594352195, 0.9764769790555311, 0.973429285080364] == [True, False, False]\n",
      "[0.8552009056878951, 0.9764843479603992, 0.9734410987062567] == [True, False, False]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    print(n.forward(data[i]), \"==\", targets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.568542403168595, 0.7694799164380262, 0.9082965936347519, 0.31381787156887975]\n",
      "[0.7656139675914392, 0.3316654602708873, 0.5032837039120778, 0.29943722220050906, 0.12284159238949688, 0.6374814007085711]\n",
      "[0.11279768404780877, 0.09040496013161836, 0.008870794417579986, 0.2904582246793722, 0.30915114801549715, 0.03339662080041339]\n",
      "0.8552009056878951\n"
     ]
    }
   ],
   "source": [
    "print(n.network[0].neurons[0].weights)\n",
    "print(n.network[1].neurons[0].weights)\n",
    "print(n.network[2].neurons[0].weights)\n",
    "\n",
    "print(n.network[2].neurons[0].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data[80][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
